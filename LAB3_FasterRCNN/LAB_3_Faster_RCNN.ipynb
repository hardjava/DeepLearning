{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "w8FxqpVNUBVZ",
   "metadata": {
    "id": "w8FxqpVNUBVZ"
   },
   "source": [
    "#LAB 3: Faster RCNN\n",
    "\n",
    "<h4><div style=\"text-align: right\"> Due date: 15:00 Nov 11, 2024.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file and final-report at PLATO before the class in the form of [ID_Name_Lab1.ipynb]. </div></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nfqy0tJJUBVc",
   "metadata": {
    "id": "nfqy0tJJUBVc"
   },
   "source": [
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span>\n",
    "- You must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>).\n",
    "- Download the dataset using the following code line;\n",
    "wget https://aistages-api-public-prod.s3.amazonaws.com/app/Competitions/000266/data/data.tar.gz\n",
    "\n",
    "- For more information, please refer (https://stages.ai/competitions/325/data/overview)\n",
    "> Copyright: CC BY 2.0\n",
    "\n",
    "### dataset\n",
    "    ├── train.json\n",
    "    ├── test.json\n",
    "    ├── train\n",
    "    └── test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9d5XT4NlBn4",
   "metadata": {
    "id": "M9d5XT4NlBn4"
   },
   "source": [
    "<h2><span style=\"color:blue\">[202255665] [송승우]</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hV9rydzjlDJy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hV9rydzjlDJy",
    "outputId": "a0dd45a4-01cc-4b5a-8c45-700cf246292a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2024-11-09 18:30:11.199640\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xsgTBr3BqE8z",
   "metadata": {
    "id": "xsgTBr3BqE8z"
   },
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "JEa9vTBhUBVd",
   "metadata": {
    "id": "JEa9vTBhUBVd"
   },
   "outputs": [],
   "source": [
    "# 권장 환경: python==3.7.13, pytorch==1.13.1, torchvision==0.14.1, albumentations==1.3.1, torchnet==0.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce329765",
   "metadata": {
    "id": "ce329765"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d76dda",
   "metadata": {
    "id": "54d76dda"
   },
   "outputs": [],
   "source": [
    "# !pip install visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c142ed1a",
   "metadata": {
    "id": "c142ed1a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import six\n",
    "from collections import namedtuple\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.ops import RoIPool\n",
    "from torchvision.ops import nms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils import data as data_\n",
    "\n",
    "from torchnet.meter import ConfusionMeter, AverageValueMeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2wb--qP79Xx2",
   "metadata": {
    "id": "2wb--qP79Xx2"
   },
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebae2a20",
   "metadata": {
    "id": "ebae2a20"
   },
   "outputs": [],
   "source": [
    "def loc2bbox(src_bbox, loc):\n",
    "    \"\"\"\n",
    "    Decodes bouding boxes from bounding box offsets and scales.\n",
    "\n",
    "    Args:\n",
    "        src_bbox: A coordinates of bounding boxes.\n",
    "            These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
    "        loc: An array with offsets and scales.\n",
    "            The shapes of 'src_bbox' and 'loc' should be same.\n",
    "            This contains values: (t_y, t_x, t_h, t_w).\n",
    "    Returns: Decoded bounding box coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # if src_bbox.shape[0] == 0:\n",
    "    #     return np.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    # src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "    # src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    # src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    # src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    # src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    # dy = loc[:, 0::4]\n",
    "    # dx = loc[:, 1::4]\n",
    "    # dh = loc[:, 2::4]\n",
    "    # dw = loc[:, 3::4]\n",
    "\n",
    "    # ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    # ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    # h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    # w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    # dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    # dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    # dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    # dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    # dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return torch.zeros((0, 4), dtype=loc.dtype, device=loc.device)\n",
    "\n",
    "    if isinstance(src_bbox, torch.Tensor):\n",
    "        src_bbox = src_bbox.cpu().numpy()\n",
    "    if isinstance(loc, torch.Tensor):\n",
    "        loc = loc.detach().cpu().numpy()\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=np.float32)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    device = loc.device if isinstance(loc, torch.Tensor) else torch.device('cpu')\n",
    "    return torch.tensor(dst_bbox, dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "\n",
    "def bbox2loc(src_bbox, dst_bbox):\n",
    "    \"\"\"\n",
    "    Encodes the source and the destination bouding boxes to \"loc\".\n",
    "\n",
    "    The offsets and scales t_y, t_x, t_h, t_w can be computed by the following formulas\n",
    "    t_y = (g_y - p_y) / p_h\n",
    "    t_x = (g_x - p_x) / p_w\n",
    "    t_h = log(g_h / p_h)\n",
    "    t_w = log(g_w / p_W)\n",
    "\n",
    "    Args:\n",
    "        src_bbox: These coordinates are (p_ymin, p_xmin, p_ymax, p_xmax).\n",
    "        dst_bbox: These coordinates are (g_ymin, g_xmin, g_ymax, g_xmax).\n",
    "\n",
    "    Returns:\n",
    "        Bounding box offsets and scales from src_bbox to dst_bbox.\n",
    "        The second axis contains four values (t_y, t_x, t_h, t_w).\n",
    "    \"\"\"\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    ctr_y = src_bbox[:, 0] + 0.5 * height\n",
    "    ctr_x = src_bbox[:, 1] + 0.5 * width\n",
    "\n",
    "    # x_min, y_min, x_max, y_max\n",
    "    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n",
    "    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n",
    "    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n",
    "    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "    eps = np.finfo(height.dtype).eps\n",
    "    height = np.maximum(height, eps)\n",
    "    width = np.maximum(width, eps)\n",
    "\n",
    "    dy = (base_ctr_y - ctr_y) / height\n",
    "    dx = (base_ctr_x - ctr_x) / width\n",
    "    dh = np.log(base_height / height)\n",
    "    dw = np.log(base_width / width)\n",
    "\n",
    "    loc = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1DTpLq7_9Qr2",
   "metadata": {
    "id": "1DTpLq7_9Qr2"
   },
   "outputs": [],
   "source": [
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initialization\n",
    "    \"\"\"\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "\n",
    "def get_inside_index(anchor, H, W):\n",
    "    # Calc indicies of anchors which are located completely inside of the image\n",
    "    # whose size is speficied.\n",
    "    index_inside = np.where(\n",
    "        (anchor[:, 0] >= 0) &\n",
    "        (anchor[:, 1] >= 0) &\n",
    "        (anchor[:, 2] <= H) &\n",
    "        (anchor[:, 3] <= W)\n",
    "    )[0]\n",
    "    return index_inside\n",
    "\n",
    "\n",
    "def unmap(data, count, index, fill=0):\n",
    "    # Unmap a subset of item (data) back to the original set of items (of size count)\n",
    "    if len(data.shape) == 1:\n",
    "        ret = np.empty((count,), dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index] = data\n",
    "    else:\n",
    "        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n",
    "        ret.fill(fill)\n",
    "        ret[index, :] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e49054b",
   "metadata": {
    "id": "7e49054b"
   },
   "outputs": [],
   "source": [
    "def tonumpy(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.detach().cpu().numpy()\n",
    "\n",
    "def totensor(data, cuda = True):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        tensor = torch.from_numpy(data)\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        tensor = data.detach()\n",
    "    if cuda:\n",
    "        tensor = tensor.cuda()\n",
    "    return tensor\n",
    "\n",
    "def scalar(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.reshape(1)[0]\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJhVDlP3qA7h",
   "metadata": {
    "id": "PJhVDlP3qA7h"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ecf14d",
   "metadata": {
    "id": "e0ecf14d"
   },
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166d1663",
   "metadata": {
    "id": "166d1663"
   },
   "outputs": [],
   "source": [
    "epochs=14\n",
    "learning_rate = 1e-3\n",
    "lr_decay = 0.1\n",
    "weight_decay = 0.0005\n",
    "# use dropout in RoIHead\n",
    "use_drop = False\n",
    "\n",
    "rpn_sigma = 3.     # sigma for l1_smooth_loss (RPN loss)\n",
    "roi_sigma = 1.     # sigma for l1_smooth_loss (ROI loss)\n",
    "\n",
    "# 데이터 경로\n",
    "data_dir = './lab3/dataset'\n",
    "# train시 checkpoint 경로\n",
    "train_load_path = None\n",
    "# inference시 체크포인트 경로\n",
    "inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fb9a7d",
   "metadata": {
    "id": "49fb9a7d"
   },
   "source": [
    "### Dataset loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093db22",
   "metadata": {
    "id": "a093db22"
   },
   "source": [
    "#### 1. TrainCustom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e08cf31",
   "metadata": {
    "id": "6e08cf31"
   },
   "outputs": [],
   "source": [
    "# TrainDataset\n",
    "class TrainCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir, transforms = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation 파일 위치\n",
    "            data_dir: data가 존재하는 폴더 경로\n",
    "            transforms : transform 여부\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        # 어노테이션 파일 로드\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_info['id'])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        # 박스 가져오기\n",
    "        boxes = np.array([x['bbox'] for x in anns])\n",
    "\n",
    "        # boxes (x_min, y_min, x_max, y_max) 꼴로 변환\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "        # 레이블 가져오기\n",
    "        labels = np.array([x['category_id'] for x in anns])\n",
    "        labels = labels.tolist() # 수정\n",
    "#        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # transform 함수 정의\n",
    "        if self.transforms :\n",
    "            scale = 1.0  # resize scale\n",
    "            H, W, _ = image.shape\n",
    "            resize_H = int(scale * H)\n",
    "            resize_W = int(scale * W)\n",
    "            transforms = get_train_transform(resize_H, resize_W)\n",
    "        else :\n",
    "            scale = 1.0\n",
    "            transforms = no_transform()\n",
    "\n",
    "        # transform\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'bboxes': boxes,\n",
    "            'labels': labels\n",
    "        }\n",
    "        sample = transforms(**sample)\n",
    "        image = sample['image']\n",
    "        bboxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "        boxes = torch.tensor(sample['bboxes'], dtype=torch.float32)\n",
    "        labels = torch.tensor(sample['labels'], dtype=torch.int64)  # 수정\n",
    "\n",
    "        # bboxes (x_min, y_min, x_max, y_max) -> boxes (y_min, x_min, y_max, x_max)\n",
    "        boxes[:, 0] = bboxes[:, 1]\n",
    "        boxes[:, 1] = bboxes[:, 0]\n",
    "        boxes[:, 2] = bboxes[:, 3]\n",
    "        boxes[:, 3] = bboxes[:, 2]\n",
    "\n",
    "        return image, boxes, labels, scale\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "856a66d5",
   "metadata": {
    "id": "856a66d5"
   },
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "class TestCustom(Dataset):\n",
    "    def __init__(self, annotation, data_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotation: annotation 파일 위치\n",
    "            data_dir: data가 존재하는 폴더 경로\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # coco annotation 불러오기 (coco API)\n",
    "        self.coco = COCO(annotation)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        # 이미지 아이디 가져오기\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "\n",
    "        # 이미지 정보 가져오기\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "\n",
    "        # 이미지 로드\n",
    "        image = cv2.imread(os.path.join(self.data_dir, image_info['file_name']))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        image = torch.tensor(image, dtype = torch.float).permute(2,0,1)\n",
    "\n",
    "        return image, image.shape[1:]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc847ee7",
   "metadata": {
    "id": "bc847ee7"
   },
   "source": [
    "#### 2. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2af672",
   "metadata": {
    "id": "6d2af672"
   },
   "outputs": [],
   "source": [
    "# Train dataset transform\n",
    "def get_train_transform(h, w):\n",
    "    return A.Compose([\n",
    "        A.Resize(height = h, width = w),\n",
    "        A.Flip(p=0.5),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "# No transform\n",
    "def no_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0) # format for pytorch tensor\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a02619",
   "metadata": {
    "id": "e8a02619"
   },
   "source": [
    "### RPN (Region Proposal Network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43368909",
   "metadata": {
    "id": "43368909"
   },
   "source": [
    "#### 1. Anchor box generator\n",
    "\n",
    "👉 mission1. anchor box 좌표값 생성\n",
    "1. 중점 만들기 (base_size의 절반)\n",
    "\n",
    "\n",
    "2. 하나의 중점당 ratio와 anchor scales에 따라 9개의 anchor box의 좌표값 만들기\\\n",
    "    anchor box의 좌표값 : (y_min, x_min, y_max, x_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2155d0ec",
   "metadata": {
    "id": "2155d0ec",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        ratios: 비율\n",
    "        anchor_scales: 스케일\n",
    "    Returns: basic anchor boxes, shape=(R, 4)\n",
    "        R: len(ratio) * len(anchor_scales) = anchor 개수 = 9\n",
    "        4: anchor box 좌표 값\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    ### ANSWER HERE ###\n",
    "    # 구현해야 할 변수 : px, py\n",
    "      # px\n",
    "      # py\n",
    "    py = base_size / 2.0\n",
    "    px = base_size / 2.0\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32) # anchor_box\n",
    "\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            # offset of anchor box\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "            ### ANSWER HERE ###1\n",
    "            # 구현해야 할 변수 : anchor_base\n",
    "            # anchor_base[index, 0]\n",
    "            # anchor_base[index, 1]\n",
    "            # anchor_base[index, 2]\n",
    "            # anchor_base[index, 3]\n",
    "            offset_x = w / 2.0\n",
    "            offset_y = h / 2.0\n",
    "            anchor_base[index, 0] = py - offset_y #y_min\n",
    "            anchor_base[index, 1] = px - offset_x #x_min\n",
    "            anchor_base[index, 2] = py + offset_y #y_max\n",
    "            anchor_base[index, 3] = px + offset_x #x_max\n",
    "\n",
    "    return anchor_base # (9,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ef20d",
   "metadata": {
    "id": "e74ef20d"
   },
   "source": [
    "#### 2. ProposalCreator\n",
    "RPN에서 구한 rpn_loc와 anchor을 통해서 Region of Interest(RoI)를 생성\\\n",
    "RoI 개수 줄이기 위해서 미리 정해둔 크기(min_size)에 맞는 roi들 중 NMS를 통해 최종 RoI 반환 (train 시 2000개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4af7541e",
   "metadata": {
    "id": "4af7541e"
   },
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    def __init__(self, parent_model,\n",
    "                 nms_thresh=0.7, # nms threshold\n",
    "                 n_train_pre_nms=12000, # train시 nms 전 roi 개수\n",
    "                 n_train_post_nms=2000, # train시 nms 후 roi 개수\n",
    "                 n_test_pre_nms=6000,   # test시 nms 전 roi 개수\n",
    "                 n_test_post_nms=300,   # test시 nms 후 roi 개수\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score, anchor, img_size, scale=1.):\n",
    "        if self.parent_model.training: # train중일 때\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else: # test중일 때\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # anchor의 좌표값과 predicted bounding bounding box offset(y,x,h,w)를 통해\n",
    "        # bounding box 좌표값(y_min, x_min, y_max, x_max) 생성\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # min_size 보다 작은 box들은 제거\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # score 텐서가 비어 있는지 확인 후, 빈 배열로 처리\n",
    "        if score.numel() > 0:\n",
    "            order = score.ravel().argsort(descending=True)\n",
    "        else:\n",
    "            order = torch.tensor([], dtype=torch.long, device=score.device)\n",
    "\n",
    "        if n_pre_nms > 0 and order.numel() > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order.cpu().numpy()]\n",
    "        score = score[order.cpu().numpy()]\n",
    "\n",
    "        # nms 적용 - detach()로 그래프 분리 후 numpy 변환\n",
    "        roi = roi if isinstance(roi, np.ndarray) else roi.detach().cpu().numpy()\n",
    "        score = score if isinstance(score, np.ndarray) else score.detach().cpu().numpy()\n",
    "\n",
    "        keep = nms(\n",
    "            torch.from_numpy(roi).cuda(),\n",
    "            torch.from_numpy(score).cuda(),\n",
    "            self.nms_thresh\n",
    "        )\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep.cpu().numpy()]\n",
    "\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a991ada",
   "metadata": {
    "id": "7a991ada"
   },
   "source": [
    "#### 3. region proposal network\n",
    "\n",
    "VGG16 통과한 feature map으로부터 region proposal들 생성\n",
    "\n",
    "👉 mission2. Region Proposal Network\n",
    " **tensor shape은 모두 1024x1024 이미지 기준입니다.**\n",
    "1. backbone에서 나온 feature map에 3x3 conv 연산을 적용하여 중간 feature map 생성 \\\n",
    "    input: x (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: middle (torch.Size([1, 512, 64, 64]))\n",
    "    \n",
    "    \n",
    "2. middle(중간 feature map)에 1x1 conv 연산을 적용하여 9x4(anchor box의 수 x bounding box 좌표값)개의 channel을 가지는 feature map 생성\\\n",
    "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: rpn_locs (torch.Size([1, 36, 64, 64]))\n",
    "    \n",
    "    \n",
    "3. middle(중간 feature map)에 1x1 conv 연산을 적용하여 9x2(anchor box의 수 x object 여부)개의 channel을 가지는 feature map 생성\\\n",
    "    input: middle (torch.Size([1, 512, 64, 64]))\\\n",
    "    output: rpn_locs (torch.Size([1, 18, 64, 64]))\n",
    "    \n",
    "    \n",
    "4. Proposal Creator 함수를 사용하여 roi 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb749c00",
   "metadata": {
    "id": "cb749c00"
   },
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32], feat_stride=16, proposal_creator_params=dict(),):\n",
    "\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "\n",
    "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios) # 9개의 anchorbox 생성\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params) # proposal_creator_params : 해당 네트워크가 training인지 testing인지 알려준다.\n",
    "        n_anchor = self.anchor_base.shape[0] # anchor 개수\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)  # 9*2\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)   # 9*4\n",
    "        normal_init(self.conv1, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.score, 0, 0.01) # weight initalizer\n",
    "        normal_init(self.loc, 0, 0.01)   # weight initalizer\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        # x(feature map)\n",
    "        n, _, hh, ww = x.shape\n",
    "\n",
    "        # 전체 (h*w*9)개 anchor의 좌표값 # anchor_base:(9, 4)\n",
    "        anchor = _enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww)\n",
    "        n_anchor = anchor.shape[0] // (hh * ww) # anchor 개수\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        ### ANSWER HERE ###\n",
    "        middle = F.relu(self.conv1(x))\n",
    "\n",
    "        # predicted bounding box offset\n",
    "        ### YOUR CODE HERE\n",
    "        ### ANSWER HERE ###\n",
    "        rpn_locs = self.loc(middle)\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "\n",
    "        # predicted scores for anchor (foreground or background)\n",
    "        ### YOUR CODE HERE\n",
    "        ### ANSWER HERE ###\n",
    "        rpn_scores = self.score(middle)\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        # scores for foreground\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
    "\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
    "\n",
    "        # proposal생성 (ProposalCreator)\n",
    "        rois = list()        # proposal의 좌표값이 있는 bounding box array\n",
    "        roi_indices = list() # roi에 해당하는 image 인덱스\n",
    "        for i in range(n):\n",
    "            ### YOUR CODE HERE\n",
    "            ### ANSWER HERE ###\n",
    "            roi = self.proposal_layer(rpn_locs[i], rpn_fg_scores[i], anchor, img_size, scale)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # anchor_base는 하나의 pixel에 9개 종류의 anchor box를 나타냄\n",
    "    # 이것을 enumerate시켜 전체 이미지의 pixel에 각각 9개의 anchor box를 가지게 함\n",
    "    # 32x32 feature map에서는 32x32x9=9216개의 anchor box가짐\n",
    "\n",
    "    shift_y = np.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = np.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shift = np.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor # (9216, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1ff0bd",
   "metadata": {
    "id": "4e1ff0bd"
   },
   "source": [
    "### Feature extractor(VGG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9911df3f",
   "metadata": {
    "id": "9911df3f"
   },
   "outputs": [],
   "source": [
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    model = vgg16(pretrained=True)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb8f86",
   "metadata": {
    "id": "c1cb8f86"
   },
   "source": [
    "### Faster R-CNN head\n",
    "\n",
    "RoI pool 후에 classifier, regressor 통과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce8ceb71",
   "metadata": {
    "id": "ce8ceb71"
   },
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Faster R-CNN head\n",
    "    RoI pool 후에 classifier, regressior 통과\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4) # bounding box regressor\n",
    "        self.score = nn.Linear(4096, n_class) # Classifier\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)  # weight initialize\n",
    "        normal_init(self.score, 0, 0.01)     # weight initialize\n",
    "\n",
    "        self.n_class = n_class # 배경 포함한 class 수\n",
    "        self.roi_size = roi_size # RoI-pooling 후 feature map의  높이, 너비\n",
    "        self.spatial_scale = spatial_scale # roi resize scale\n",
    "        self.roi = RoIPool( (self.roi_size, self.roi_size),self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = totensor(roi_indices).float()\n",
    "        rois = totensor(rois).float()\n",
    "        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
    "\n",
    "        # 각 이미지 roi pooling\n",
    "        pool = self.roi(x, indices_and_rois)\n",
    "        # flatten\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        # fully connected\n",
    "        fc7 = self.classifier(pool)\n",
    "        # regression\n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        # softmax\n",
    "        roi_scores = self.score(fc7)\n",
    "\n",
    "\n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cec189",
   "metadata": {
    "id": "85cec189"
   },
   "source": [
    "### Faster R-CNN\n",
    "Feature Extraction : image로부터 feature map 생성\\\n",
    "Region Proposal Networks : Region of Interest 생성\\\n",
    "Localization and Classification Head : RoI에 해당하는 feature map을 최종 detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a04aba6",
   "metadata": {
    "id": "0a04aba6"
   },
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    def new_f(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return f(*args, **kwargs)\n",
    "    return new_f\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor  # extractor : vgg\n",
    "        self.rpn = rpn              # rpn : region proposal network\n",
    "        self.head = head            # head : RoiHead\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset()\n",
    "\n",
    "    @property\n",
    "    def n_class(self): # 최종 class 개수 (배경 포함)\n",
    "        return self.head.n_class\n",
    "\n",
    "    # predict 시 사용하는 forward\n",
    "    # train 시 FasterRCNNTrainer을 사용하여 FasterRcnn에 있는 extractor, rpn, head를 모듈별로 불러와서 forward\n",
    "    def forward(self, x, scale=1.):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x) # extractor 통과\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(h, img_size, scale) # rpn 통과\n",
    "        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices) # head 통과\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
    "\n",
    "    def use_preset(self): # prediction 과정 쓰이는 threshold 정의\n",
    "        self.nms_thresh = 0.3\n",
    "        self.score_thresh = 0.05\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "\n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
    "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep].cpu().numpy())\n",
    "\n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    @nograd\n",
    "    def predict(self, imgs,sizes=None):\n",
    "        \"\"\"\n",
    "        이미지에서 객체 검출\n",
    "        Input : images\n",
    "        Output : bboxes, labels, scores\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        prepared_imgs = imgs\n",
    "\n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale) # self = FasterRCNN\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = torch.Tensor(self.loc_normalize_mean).cuda(). repeat(self.n_class)[None]\n",
    "            std = torch.Tensor(self.loc_normalize_std).cuda(). repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(tonumpy(roi).reshape((-1, 4)),tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = (F.softmax(totensor(roi_score), dim=1))\n",
    "\n",
    "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset()\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        '''\n",
    "        Optimizer 선언\n",
    "        '''\n",
    "        lr = learning_rate\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': weight_decay}]\n",
    "        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39849b21",
   "metadata": {
    "id": "39849b21"
   },
   "source": [
    "### Faster R-CNN 생성\n",
    "Extractor(VGG) + RPN + Head 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32726880",
   "metadata": {
    "id": "32726880"
   },
   "outputs": [],
   "source": [
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self, n_fg_class=10, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32] ): # n_fg_class : 배경포함 하지 않은 class 개수\n",
    "        extractor, classifier = decom_vgg16()\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46202f13",
   "metadata": {
    "id": "46202f13"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61acb16a",
   "metadata": {
    "id": "61acb16a"
   },
   "source": [
    "#### 0. util 함수 정의\n",
    "bounding box IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9d25d7e",
   "metadata": {
    "id": "c9d25d7e"
   },
   "outputs": [],
   "source": [
    "def bbox_iou(bbox_a, bbox_b):\n",
    "    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    #bbox_a 1개와 bbox_b k개를 비교해야하므로 None을 이용해서 차원을 늘려서 연산한다.\n",
    "    # top left\n",
    "    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n",
    "    # bottom right\n",
    "    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n",
    "\n",
    "    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n",
    "    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n",
    "    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3ce234",
   "metadata": {
    "id": "ac3ce234"
   },
   "source": [
    "#### 1. Anchor Target Creator\n",
    "Anchor box에 해당하는 ground truth bounding box match\\\n",
    "Region Proposal Network loss 구할 때 ground truth로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69a1e360",
   "metadata": {
    "id": "69a1e360"
   },
   "outputs": [],
   "source": [
    "class AnchorTargetCreator(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_sample=256,\n",
    "                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n",
    "                 pos_ratio=0.5):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_iou_thresh = pos_iou_thresh\n",
    "        self.neg_iou_thresh = neg_iou_thresh\n",
    "        self.pos_ratio = pos_ratio\n",
    "\n",
    "    def __call__(self, bbox, anchor, img_size):\n",
    "\n",
    "        img_H, img_W = img_size\n",
    "\n",
    "        n_anchor = len(anchor) # 9216\n",
    "        inside_index = get_inside_index(anchor, img_H, img_W) # (2272,)\n",
    "        anchor = anchor[inside_index] # (2272, 4)\n",
    "        argmax_ious, label = self._create_label(\n",
    "            inside_index, anchor, bbox)\n",
    "\n",
    "        # compute bounding box regression targets\n",
    "        loc = bbox2loc(anchor, bbox[argmax_ious]) # (2272, 4)\n",
    "\n",
    "        # map up to original set of anchors\n",
    "        label = unmap(label, n_anchor, inside_index, fill=-1) # (9216,)\n",
    "        loc = unmap(loc, n_anchor, inside_index, fill=0) # (9216, 4)\n",
    "\n",
    "        return loc, label\n",
    "\n",
    "    def _create_label(self, inside_index, anchor, bbox):\n",
    "        # label) 1 :positive, 0 : negative, -1 : dont care\n",
    "        label = np.empty((len(inside_index),), dtype=np.int32)\n",
    "        label.fill(-1)\n",
    "\n",
    "        argmax_ious, max_ious, gt_argmax_ious = self._calc_ious(anchor, bbox, inside_index)\n",
    "\n",
    "        label[max_ious < self.neg_iou_thresh] = 0 # 0.3\n",
    "\n",
    "        # 가장 iou가 큰 것은 positive label\n",
    "        label[gt_argmax_ious] = 1\n",
    "\n",
    "        # positive label\n",
    "        label[max_ious >= self.pos_iou_thresh] = 1 # 0.7\n",
    "\n",
    "        # subsample positive labels if we have too many\n",
    "        n_pos = int(self.pos_ratio * self.n_sample)\n",
    "        pos_index = np.where(label == 1)[0]\n",
    "        if len(pos_index) > n_pos:\n",
    "            disable_index = np.random.choice(\n",
    "                pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        # subsample negative labels if we have too many\n",
    "        n_neg = self.n_sample - np.sum(label == 1)\n",
    "        neg_index = np.where(label == 0)[0]\n",
    "        if len(neg_index) > n_neg:\n",
    "            disable_index = np.random.choice(\n",
    "                neg_index, size=(len(neg_index) - n_neg), replace=False)\n",
    "            label[disable_index] = -1\n",
    "\n",
    "        return argmax_ious, label\n",
    "\n",
    "    def _calc_ious(self, anchor, bbox, inside_index):\n",
    "        # ious between the anchors and the gt boxes\n",
    "        ious = bbox_iou(anchor, bbox)\n",
    "        argmax_ious = ious.argmax(axis=1)\n",
    "        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n",
    "        gt_argmax_ious = ious.argmax(axis=0)\n",
    "        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "\n",
    "        return argmax_ious, max_ious, gt_argmax_ious"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11881f",
   "metadata": {
    "id": "9a11881f"
   },
   "source": [
    "#### 2. positive, negative sampling\n",
    "RPN에서 NMS를 거친 roi들을 ground truth와의 iou를 비교\\\n",
    "positive / negative sampling 수행 (총 128개)\\\n",
    "sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h)을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6928b27",
   "metadata": {
    "id": "e6928b27"
   },
   "outputs": [],
   "source": [
    "class ProposalTargetCreator:\n",
    "    def __init__(self,\n",
    "                 n_sample=128,\n",
    "                 pos_ratio=0.25, pos_iou_thresh=0.5,\n",
    "                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n",
    "                 ):\n",
    "        self.n_sample = n_sample\n",
    "        self.pos_ratio = pos_ratio\n",
    "        self.pos_iou_thresh = pos_iou_thresh # positive iou threshold\n",
    "        self.neg_iou_thresh_hi = neg_iou_thresh_hi # negitave iou threshold = (neg_iou_thresh_hi ~ neg_iou_thresh_lo)\n",
    "        self.neg_iou_thresh_lo = neg_iou_thresh_lo\n",
    "\n",
    "    def __call__(self, roi, bbox, label,\n",
    "                 loc_normalize_mean=(0., 0., 0., 0.),\n",
    "                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n",
    "        n_bbox, _ = bbox.shape\n",
    "\n",
    "        roi = np.concatenate((roi, bbox), axis=0)\n",
    "\n",
    "        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio) # positive image 갯수 = 32\n",
    "        iou = bbox_iou(roi, bbox) # RoI와 bounding box IoU\n",
    "        gt_assignment = iou.argmax(axis=1)\n",
    "        max_iou = iou.max(axis=1)\n",
    "        gt_roi_label = label[gt_assignment] + 1 # class label [0, n_fg_class - 1] -> [1, n_fg_class].\n",
    "\n",
    "        # positive sample 선택 (>= pos_iou_thresh IoU)\n",
    "        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "        if pos_index.size > 0:\n",
    "            pos_index = np.random.choice(\n",
    "                pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "\n",
    "        # Negative sample 선택 [neg_iou_thresh_lo, neg_iou_thresh_hi)\n",
    "        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n",
    "                             (max_iou >= self.neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                         neg_index.size))\n",
    "        if neg_index.size > 0:\n",
    "            neg_index = np.random.choice(\n",
    "                neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "\n",
    "        # The indices that we're selecting (both positive and negative).\n",
    "        keep_index = np.append(pos_index, neg_index)\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative sample의 label = 0\n",
    "        sample_roi = roi[keep_index] # (128, 4)\n",
    "\n",
    "        # sample roi와 gt_bbox를 이용해 bbox regression에서 regression해야할 ground truth loc값(t_x, t_y, t_w, t_h) 계산\n",
    "        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]]) # (128, 4)\n",
    "        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)) / np.array(loc_normalize_std, np.float32))\n",
    "\n",
    "        return sample_roi, gt_roi_loc, gt_roi_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2da581",
   "metadata": {
    "id": "ec2da581"
   },
   "source": [
    "#### 3. Trainer 정의\n",
    "training, loss 계산, checkpoint 저장 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d40ccd0b",
   "metadata": {
    "id": "d40ccd0b"
   },
   "outputs": [],
   "source": [
    "LossTuple = namedtuple('LossTuple', ['rpn_loc_loss', 'rpn_cls_loss',\n",
    "                                     'roi_loc_loss', 'roi_cls_loss',\n",
    "                                     'total_loss'])\n",
    "class FasterRCNNTrainer(nn.Module):\n",
    "\n",
    "    def __init__(self, faster_rcnn):\n",
    "        super(FasterRCNNTrainer, self).__init__()\n",
    "\n",
    "        self.faster_rcnn = faster_rcnn\n",
    "        self.rpn_sigma = rpn_sigma\n",
    "        self.roi_sigma = roi_sigma\n",
    "\n",
    "        # target creator create gt_bbox gt_label etc as training targets.\n",
    "        self.anchor_target_creator = AnchorTargetCreator()\n",
    "        self.proposal_target_creator = ProposalTargetCreator()\n",
    "\n",
    "        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n",
    "        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n",
    "\n",
    "        self.optimizer = self.faster_rcnn.get_optimizer()\n",
    "\n",
    "        # training 상태 보여주는 지표\n",
    "        self.rpn_cm = ConfusionMeter(2) # confusion matrix for classification\n",
    "        self.roi_cm = ConfusionMeter(11)  # confusion matrix for classification\n",
    "        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss\n",
    "\n",
    "    def forward(self, imgs, bboxes, labels, scale):\n",
    "        n = bboxes.shape[0]\n",
    "\n",
    "        if n != 1:\n",
    "            raise ValueError('Currently only batch size 1 is supported.')\n",
    "\n",
    "        _, _, H, W = imgs.shape\n",
    "        img_size = (H, W)\n",
    "\n",
    "        # VGG (features extractor)\n",
    "        features = self.faster_rcnn.extractor(imgs)\n",
    "\n",
    "        # RPN (region proposal)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, img_size, scale)\n",
    "\n",
    "        # Since batch size is one, convert variables to singular form\n",
    "        bbox = bboxes[0]\n",
    "        label = labels[0]\n",
    "        rpn_score = rpn_scores[0]\n",
    "        rpn_loc = rpn_locs[0]\n",
    "        roi = rois\n",
    "\n",
    "        \"\"\"\n",
    "        sample roi =  rpn에서 nms 거친 2000개의 roi들 중 positive/negative 비율 고려해 최종 sampling한 roi\n",
    "        \"\"\"\n",
    "        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n",
    "            roi,\n",
    "            tonumpy(bbox),\n",
    "            tonumpy(label),\n",
    "            self.loc_normalize_mean,\n",
    "            self.loc_normalize_std)\n",
    "\n",
    "        # NOTE it's all zero because now it only support for batch=1 now\n",
    "        # Faster R-CNN head (prediction head)\n",
    "        sample_roi_index = torch.zeros(len(sample_roi))\n",
    "        roi_cls_loc, roi_score = self.faster_rcnn.head(features,sample_roi,sample_roi_index)\n",
    "\n",
    "        # ------------------ RPN losses -------------------#\n",
    "        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox),anchor,img_size)\n",
    "        gt_rpn_label = totensor(gt_rpn_label).long()\n",
    "        gt_rpn_loc = totensor(gt_rpn_loc)\n",
    "\n",
    "        # rpn bounding box regression loss\n",
    "        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_loc,gt_rpn_loc,gt_rpn_label.data,self.rpn_sigma)\n",
    "        # rpn classification loss\n",
    "        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n",
    "\n",
    "        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n",
    "        _rpn_score = tonumpy(rpn_score)[tonumpy(gt_rpn_label) > -1]\n",
    "        self.rpn_cm.add(totensor(_rpn_score, False), _gt_rpn_label.data.long())\n",
    "\n",
    "        # ------------------ ROI losses (fast rcnn loss) -------------------#\n",
    "        n_sample = roi_cls_loc.shape[0]\n",
    "        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "        roi_loc = roi_cls_loc[torch.arange(0, n_sample).long().cuda(), \\\n",
    "                              totensor(gt_roi_label).long()]\n",
    "        gt_roi_label = totensor(gt_roi_label).long()\n",
    "        gt_roi_loc = totensor(gt_roi_loc)\n",
    "\n",
    "        # faster rcnn bounding box regression loss\n",
    "        roi_loc_loss = _fast_rcnn_loc_loss(\n",
    "            roi_loc.contiguous(),\n",
    "            gt_roi_loc,\n",
    "            gt_roi_label.data,\n",
    "            self.roi_sigma)\n",
    "\n",
    "        # faster rcnn classification loss\n",
    "        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n",
    "\n",
    "        self.roi_cm.add(totensor(roi_score, False), gt_roi_label.data.long())\n",
    "\n",
    "        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n",
    "        losses = losses + [sum(losses)] # total_loss == sum(losses)\n",
    "\n",
    "        return LossTuple(*losses)\n",
    "\n",
    "    # training\n",
    "    def train_step(self, imgs, bboxes, labels, scale):\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = self.forward(imgs, bboxes, labels, scale)\n",
    "        losses.total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.update_meters(losses)\n",
    "        return losses\n",
    "\n",
    "    # checkpoint 만들기\n",
    "    def save(self, save_optimizer=False, save_path=None):\n",
    "        save_dict = dict()\n",
    "\n",
    "        save_dict['model'] = self.faster_rcnn.state_dict()\n",
    "\n",
    "        if save_optimizer:\n",
    "            save_dict['optimizer'] = self.optimizer.state_dict()\n",
    "\n",
    "        if save_path is None:\n",
    "            save_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'\n",
    "\n",
    "        save_dir = os.path.dirname(save_path)\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        torch.save(save_dict, save_path)\n",
    "        return save_path\n",
    "\n",
    "    # checkpoint load\n",
    "    def load(self, path, load_optimizer=True, parse_opt=False, ):\n",
    "        state_dict = torch.load(path)\n",
    "        if 'model' in state_dict:\n",
    "            self.faster_rcnn.load_state_dict(state_dict['model'])\n",
    "        else:  # legacy way, for backward compatibility\n",
    "            self.faster_rcnn.load_state_dict(state_dict)\n",
    "            return self\n",
    "        if 'optimizer' in state_dict and load_optimizer:\n",
    "            self.optimizer.load_state_dict(state_dict['optimizer'])\n",
    "        return self\n",
    "\n",
    "    def update_meters(self, losses):\n",
    "        loss_d = {k: scalar(v) for k, v in losses._asdict().items()}\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.add(loss_d[key])\n",
    "\n",
    "    def reset_meters(self):\n",
    "        for key, meter in self.meters.items():\n",
    "            meter.reset()\n",
    "        self.roi_cm.reset()\n",
    "        self.rpn_cm.reset()\n",
    "\n",
    "    def get_meter_data(self):\n",
    "        return {k: v.value()[0] for k, v in self.meters.items()}\n",
    "\n",
    "\n",
    "def _smooth_l1_loss(x, t, in_weight, sigma):\n",
    "    sigma2 = sigma ** 2\n",
    "    diff = in_weight * (x - t)\n",
    "    abs_diff = diff.abs()\n",
    "    flag = (abs_diff.data < (1. / sigma2)).float()\n",
    "    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n",
    "         (1 - flag) * (abs_diff - 0.5 / sigma2))\n",
    "    return y.sum()\n",
    "\n",
    "\n",
    "def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n",
    "    # Localization loss 구할 때는 positive example에 대해서만 계산\n",
    "    in_weight = torch.zeros(gt_loc.shape).cuda()\n",
    "    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n",
    "    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n",
    "    loc_loss /= ((gt_label >= 0).sum().float())\n",
    "    return loc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03eee6",
   "metadata": {
    "id": "2b03eee6"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c9224c3",
   "metadata": {
    "id": "9c9224c3"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Train dataset 불러오기\n",
    "#     dataset = TrainDataset()\n",
    "    annotation = os.path.join(data_dir,'train.json')\n",
    "    dataset = TrainCustom(annotation, data_dir, transforms=True)\n",
    "    print('load data')\n",
    "    dataloader = data_.DataLoader(dataset,\n",
    "                                  batch_size=1,     # only batch_size=1 support\n",
    "                                  shuffle=True,\n",
    "                                  pin_memory=False,\n",
    "                                  num_workers=0)\n",
    "\n",
    "    # faster rcnn 불러오기\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    print('model construct completed')\n",
    "\n",
    "    # faster rcnn trainer 불러오기\n",
    "    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n",
    "\n",
    "    # checkpoint load\n",
    "    if train_load_path:\n",
    "        trainer.load(train_load_path)\n",
    "        print('load pretrained model from %s' % train_load_path)\n",
    "\n",
    "    lr_ = learning_rate\n",
    "    best_loss = 1000\n",
    "    for epoch in range(epochs):\n",
    "        trainer.reset_meters()\n",
    "        for ii, (img, bbox_, label_, scale) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n",
    "            trainer.train_step(img, bbox, label, float(scale))\n",
    "\n",
    "        losses = trainer.get_meter_data()\n",
    "        print(f\"Epoch #{epoch+1} loss: {losses}\")\n",
    "        if losses['total_loss'] < best_loss :\n",
    "            trainer.save()\n",
    "\n",
    "        if epoch == 9:\n",
    "            trainer.faster_rcnn.scale_lr(lr_decay)\n",
    "            lr_ = lr_ * lr_decay\n",
    "\n",
    "        if epoch == 13:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d704420",
   "metadata": {
    "id": "1d704420",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "load data\n",
      "model construct completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [25:12<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 loss: {'rpn_loc_loss': 0.16296565212038538, 'rpn_cls_loss': 0.2930995408135084, 'roi_loc_loss': 0.3066282464249195, 'roi_cls_loss': 0.5293878217479716, 'total_loss': 1.2920812604101655}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [26:27<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2 loss: {'rpn_loc_loss': 0.15204933872677803, 'rpn_cls_loss': 0.23568887747107065, 'roi_loc_loss': 0.2649910874822034, 'roi_cls_loss': 0.4775334987262984, 'total_loss': 1.1302628014133669}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [30:06<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3 loss: {'rpn_loc_loss': 0.14819781538640509, 'rpn_cls_loss': 0.21723356735271815, 'roi_loc_loss': 0.24510826337270378, 'roi_cls_loss': 0.4476282269509483, 'total_loss': 1.058167873683976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [32:07<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4 loss: {'rpn_loc_loss': 0.14389519814267643, 'rpn_cls_loss': 0.20552195079022706, 'roi_loc_loss': 0.2319121240636047, 'roi_cls_loss': 0.42352939652366367, 'total_loss': 1.004858669028609}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [33:27<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 loss: {'rpn_loc_loss': 0.14203214383224783, 'rpn_cls_loss': 0.196276060198728, 'roi_loc_loss': 0.22375927160853143, 'roi_cls_loss': 0.4092678862802962, 'total_loss': 0.9713353620273361}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [33:27<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6 loss: {'rpn_loc_loss': 0.14051018879086966, 'rpn_cls_loss': 0.18920566707493974, 'roi_loc_loss': 0.21502925108789417, 'roi_cls_loss': 0.38825755814418883, 'total_loss': 0.9330026655519535}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [34:17<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7 loss: {'rpn_loc_loss': 0.13932230719777305, 'rpn_cls_loss': 0.18526345476379927, 'roi_loc_loss': 0.20938612446024873, 'roi_cls_loss': 0.377038203042893, 'total_loss': 0.9110100893637544}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [31:36<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8 loss: {'rpn_loc_loss': 0.13760680140418685, 'rpn_cls_loss': 0.17844945467108722, 'roi_loc_loss': 0.20369465495433747, 'roi_cls_loss': 0.3643273768403721, 'total_loss': 0.8840782869421839}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [30:50<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9 loss: {'rpn_loc_loss': 0.1358492874811153, 'rpn_cls_loss': 0.1729374675179549, 'roi_loc_loss': 0.200087181242396, 'roi_cls_loss': 0.35628934684525143, 'total_loss': 0.8651632829698025}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [27:14<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10 loss: {'rpn_loc_loss': 0.13454444859020767, 'rpn_cls_loss': 0.17022916905012803, 'roi_loc_loss': 0.1961237027193201, 'roi_cls_loss': 0.3458390356265221, 'total_loss': 0.8467363551695396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [25:58<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11 loss: {'rpn_loc_loss': 0.12590865779601484, 'rpn_cls_loss': 0.14859050182202568, 'roi_loc_loss': 0.17749457615039466, 'roi_cls_loss': 0.28692736170351396, 'total_loss': 0.738921098243241}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [22:23<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12 loss: {'rpn_loc_loss': 0.12551654701026202, 'rpn_cls_loss': 0.14372694592209767, 'roi_loc_loss': 0.1741620293743932, 'roi_cls_loss': 0.27149811041883354, 'total_loss': 0.7149036326034621}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [22:22<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13 loss: {'rpn_loc_loss': 0.12389424580604211, 'rpn_cls_loss': 0.14156891748343678, 'roi_loc_loss': 0.17325980001600824, 'roi_cls_loss': 0.2644691040332029, 'total_loss': 0.7031920669429083}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4883/4883 [22:48<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14 loss: {'rpn_loc_loss': 0.12423923771147548, 'rpn_cls_loss': 0.14000104283977424, 'roi_loc_loss': 0.17126508781443456, 'roi_cls_loss': 0.26111459673359355, 'total_loss': 0.6966199647935051}\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0771b1d",
   "metadata": {
    "id": "b0771b1d"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43a67b27",
   "metadata": {
    "id": "43a67b27"
   },
   "outputs": [],
   "source": [
    "def eval(dataloader, faster_rcnn):\n",
    "    outputs = []\n",
    "    for ii, (imgs, sizes) in enumerate(tqdm(dataloader)):\n",
    "        sizes = [sizes[0][0].item(), sizes[1][0].item()]\n",
    "        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n",
    "        for out in range(len(pred_bboxes_)):\n",
    "            outputs.append({'boxes':pred_bboxes_[out], 'scores': pred_scores_[out], 'labels': pred_labels_[out]})\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c77514fc",
   "metadata": {
    "id": "c77514fc"
   },
   "outputs": [],
   "source": [
    "def inference():\n",
    "\n",
    "    # Test dataset 불러오기\n",
    "#     testset = TestDataset()\n",
    "    annotation = os.path.join(data_dir,'test.json')\n",
    "    testset = TestCustom(annotation, data_dir)\n",
    "    test_dataloader = data_.DataLoader(testset,\n",
    "                                       batch_size=1, # only batch_size=1 support\n",
    "                                       num_workers=0,\n",
    "                                       shuffle=False,\n",
    "                                       pin_memory=False\n",
    "                                       )\n",
    "    # faster rcnn 불러오기\n",
    "    faster_rcnn = FasterRCNNVGG16().cuda()\n",
    "    state_dict = torch.load(inf_load_path)\n",
    "    if 'model' in state_dict:\n",
    "        faster_rcnn.load_state_dict(state_dict['model'])\n",
    "    print('load pretrained model from %s' % inf_load_path)\n",
    "\n",
    "    # evaluation\n",
    "    outputs = eval(test_dataloader, faster_rcnn)\n",
    "    score_threshold = 0.05\n",
    "    prediction_strings = []\n",
    "    file_names = []\n",
    "\n",
    "    # submission file 작성\n",
    "    coco = COCO(os.path.join(data_dir, 'test.json'))\n",
    "    for i, output in enumerate(outputs):\n",
    "        prediction_string = ''\n",
    "        image_info = coco.loadImgs(coco.getImgIds(imgIds=i))[0]\n",
    "        for box, score, label in zip(output['boxes'], output['scores'], output['labels']):\n",
    "            if score > score_threshold:\n",
    "                prediction_string += str(label) + ' ' + str(score) + ' ' + str(box[1]) + ' ' + str(\n",
    "                    box[0]) + ' ' + str(box[3]) + ' ' + str(box[2]) + ' '\n",
    "        prediction_strings.append(prediction_string)\n",
    "        file_names.append(image_info['file_name'])\n",
    "    submission = pd.DataFrame()\n",
    "    submission['PredictionString'] = prediction_strings\n",
    "    submission['image_id'] = file_names\n",
    "    submission.to_csv(\"./faster_rcnn_scratch_submission.csv\", index=False)\n",
    "\n",
    "    print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06b16e50",
   "metadata": {
    "id": "06b16e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "load pretrained model from ./checkpoints/faster_rcnn_scratch_checkpoints.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4871/4871 [15:26<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "                                    PredictionString       image_id\n",
      "0  0 0.19532865 227.24829 682.7295 303.48328 744....  test/0000.jpg\n",
      "1  0 0.123591594 125.164185 7.157425 486.56708 28...  test/0001.jpg\n",
      "2  0 0.6346706 299.16876 40.097244 331.43622 189....  test/0002.jpg\n",
      "3  0 0.071558595 0.0 38.370667 253.69162 654.4165...  test/0003.jpg\n",
      "4  0 0.59937394 204.89127 324.0589 737.99817 752....  test/0004.jpg\n"
     ]
    }
   ],
   "source": [
    "inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8b59f",
   "metadata": {
    "id": "c4f8b59f"
   },
   "source": [
    "# Reference\n",
    "https://github.com/chenyuntc/simple-faster-rcnn-pytorch \\\n",
    "https://github.com/shkim960520/faster-rcnn-for-studying"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
